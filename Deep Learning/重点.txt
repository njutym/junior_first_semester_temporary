不考代码， 记得带计算器

第一章 绪论：

P12 深度学习关系  机器人有一些机械设计在外面，不属于人工智能；想好各个圈关系

P14 P15 机器学习概念  语义鸿沟定义   
 
P15一些核心问题，都很重要

P21 反向传播

P22 第一个卷积神经网络，干啥的，贡献是什么

P24 大规模数据集要记住类别和图片数目量级

P26 第一个深度卷积网络Alenet时间点，训练方式

P28 深度学习三个关键点

P53 不同任务间的共性


第二章 线性分类器

 P4 图像分类是最核心的任务

P5 语义鸿沟 为什么

P6 紧接着的几页   视角差异 光照变化等等这几个挑战

P14 图像分类器无法硬编码，过拟合

P16 数据驱动

P18 KNN 做法

P20 距离度量

P21 时间复杂度

P22 1近邻产生的问题，离群点无法很好地分类

P23 为什么有白色区域

P38 线性分类器定义，公式，形状

P45 如何理解线性分类器（代数视角（矩阵乘法），视觉视角（学到模板进行匹配），几何视角（超平面划分空间））

P49 线性分类器难以处理的情况

P50 非线性变换

P51 会给一个小的例子，具体计算, SVM损失要记住

P62开始的几页 损失函数的一些问题

P70 softmax公式

P78 上面的一些问题，要理解，不要硬记


第三章 正则化与优化

P7 为什么有正则化？W是独一无二的吗？

P11 正则化公式

P14 正则化特点

P17 几种正则化方式 ，lambda实际要除以N

P18 其它方法

P19 为什么正则化

P20 不同正则化的偏好

P29 梯度值是什么，方向是干什么的

P30 梯度下降流程

P41 数值法和解析法对比， 梯度检查

P42 随机梯度下降法，考虑计算资源和成本，小批量样本计算

P43 流程

P45 开始的一些页  SGD一些问题理解一下

P50 如何区分鞍点和最优值

P54开始的一些页   的各种想法，要理解，比如为什么要加动量，为什么要归一化，实现了什么样的效果

P61 两者区别

P65 权重衰减

P69 图形曲线很重要

P71 学习率设置方法了解就行

P77 一阶优化使用梯度

P80 优化复杂度



第四章 神经网络与反向传播

P5 为什么需要非线性

P7 二层神经网络什么样子

P10 W1 100个类别共享的模板理解： 一个w提取线特征，一个提取面特征，每个W都提取一种特征

P15 激活函数公式和性质 其中maxout，elu不需要记住

P18 代码流程

P23 P24 神经网络大小带来的影响 

！！！！
P33 计算图和反向传播 非常重要 非常重要 非常重要 不考矩阵的

P62 具体例子

P74 sigmoid 可以变为整体来做

P81 基本计算模式

代码不考
！！！！

P95 向量梯度了解，记住形状大小关系，不会要计算


第五章 卷积神经网络

P43 全连接层参数量大，没有空间信息 

P44 卷积层特点，保持空间结构

P46 滤波器形状

P52 特征激活图形状

P54 输入输出对应关系

P57 输入输出对应关系

P64 浅层滤波器学到了什么东西（局部图像模板（边缘，颜色，纹理等等））

P84 输出公式大小，要理解

P92 各种数值， 参数量，注意参数量要加上bias

P93 感受野很重要，怎么计算，要计算不同步长下的，审题要注意是在前一层上的感受野，还是在输入图上的感受野

P98 参数量公式

P103 理解全连接层，理解每个神经元在做什么？每个神经元都看到整个输入

P107 池化层 每个通道单独处理，独立操作，没有可学习的参数，输出大小计算同卷积计算，一般不加padding

P113 1*1卷积主要作用：特征图通道的变化，增加非线性，计算量和参数量最低

P114开始几页 3D卷积、转置卷积、空洞卷积、分组卷积、可分离、可变性卷积概念即可

第六章 卷积神经网络架构

！！！！！！！！！！！！！
P6 batch normalization 很重要 很重要 很重要

P10 计算方式

P11 训练参数如何计算，测试如何估计，\beta \gamma哪里来的 BN层强烈建议找网上的文献或者视频，解读BN层性质，到底在干啥

P16 BN 层好处和作用，理解

P17 了解不同的归一层，可以了解网上工作
！！！！！！！！！！！！！


P28 不需要记住网络结构， 使用relu，使用gpu，其它不要记住

P31 VGG 更小的滤波器，更深的网络 ，使用的是什么，最常用的是16，以及19

P33 为什么只使用3*3卷积核，相同感受野，更少参数量，更多非线性

P39 显存和参数量该怎么算，只需要知道公式

P41 瓶颈在哪里

P43 Googlenet贡献，inception模块

P46 两个问题，如何解决

P56 resnet在做什么，解决了什么问题

P57 图

P60 理解问题

P61 做法，拟合什么东西

P62 bottleneck

P64 四种网络分别代表了什么（里程碑）

P65 SEnet在做什么，在做通道的一个注意力，对每个通道缩放，求一个scale，传上去

P70 迁移学习理解

P73 不同大小数据集如何迁移



第七章 训练神经网络

P3 感受野公式，不同层的区别

P4 取整方式

P11开始几页 激活函数问题（sigmoid，relu）以及特性，后面erelu那里不考

P26 输出以0为中心的好处

P27 使用建议

P30 数据预处理方法

P46开始几页 dropout理解，测试的时候需要做什么事情，dropout可以看作模型集成

P55 跳层

P70 P74 超参数选择步骤

第八章 循环神经网络

P9 公式理解

P12 共享参数

P41 RNN 优势和缺点

P59 RNN 梯度计算

P72 LSTM 中间的图需要知道，P84公式也需要记住 

P88 LSTM解决梯度消失问题了吗？

P97 总结


第九章 注意力机制与 Transformer

P13 注意力类型，知道有不同的哪些类型即可

P16 P17 一些特点

P19 如何计算自注意力   

P24 多头注意力了解即可

P26 左边不需要记住，右边图需要记住

P61 了解如何划分，channel除以h，每个head除以一组，并起来

P66 前向层是MLP，有res和norm

P92 视觉ViT 需要记住图	


第十章 目标检测与图像分割

P8 P11 滑窗计算的问题，解决方案

P13 全卷积，问题

P16 解决方案

P17 U-net encoder和decoder大概怎么做 细节不需要记住

P18 encode-only怎么做 细节不需要记住

P19 pspnet 细节不需要记住

P20 对图像的每个像素进行分类  不区分不同物体，只关心像素

P30 知道有Region Proposals: Selective Search 这个东西即可

P31开始几页 RCNN流程

P35 P36 Wrap 为什么要调整大小，因为要批量处理

P38 问题和解决方案

P42 Fast R-CNN是如何做的

P50 Faster R-CNN如何做

P51 Region Proposal Network的概念需要了解

P57 四个损失

P60 二阶段检测器

P71 Mask R-CNN 知道怎么做，考试不一定考


第十一章 视频理解

P7 问题和解决方案

P10 大概怎么做的

P12开始 几个 late fusion，early fusion只需要知道大概怎么做就行

P19 3D卷积

P30 3D卷积特性

P40 双流网络知道是哪双流

P52 Recurrent Convolutional 只需要了解概念，不需要记公式

P66 I3D，如何替换，想让2D和3D结果相同，重用2D处理3D任务


第十二章 可视化

P57 格拉姆矩阵 用来干什么，怎么用的

第十三章 自监督学习

P2 定义

P6 如何评估方法

P9开始几页 大概有什么类型的预测目标，需要知道几个

P37 三点需要知道

P43 P44 对比学习形式化定义，概念



第十四章 视觉-语言模型

P16 模型预训练方式 四类

P17 全监督在做什么，缺点

P19 语言-图像对比学习在做什么

P19 P21 CLIP设计的关键，是怎么做的

P54 通用视觉模型的尝试

P59 三个块分别怎么去做

P65 分割任务有哪些

P79 任务粒度有哪些，举例子

